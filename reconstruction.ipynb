{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from BigGAN import Generator\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor, Compose, Resize\n",
    "from latentaug.truncated import TruncatedResNet18\n",
    "from latentaug.encoder import ApproximateEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10('.', train=True, transform=Compose([ToTensor()]))#, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img(out):\n",
    "    return np.transpose(out.detach().cpu(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedGenerator(nn.Module):\n",
    "    def __init__(self, G, z_init=None, device='cuda:1', batch_size=1):\n",
    "        super(InvertedGenerator, self).__init__()\n",
    "        self.G = G\n",
    "        self.batch_size = batch_size\n",
    "        self.z_init = torch.FloatTensor(self.batch_size, 128).normal_(0, 1).to(device) if z_init is None else z_init\n",
    "        self.delta = [\n",
    "            Variable(self.z_init, requires_grad=True),\n",
    "            #Variable(torch.FloatTensor(self.batch_size, 4096).zero_().to(device), requires_grad=True),\n",
    "            #Variable(torch.FloatTensor(self.batch_size, 256, 4, 4).zero_().to(device), requires_grad=True),\n",
    "            #Variable(torch.FloatTensor(self.batch_size, 256, 8, 8).zero_().to(device), requires_grad=True),\n",
    "            #Variable(torch.FloatTensor(self.batch_size, 256, 16, 16).zero_().to(device), requires_grad=True),\n",
    "            #Variable(torch.FloatTensor(self.batch_size, 256, 32, 32).zero_().to(device), requires_grad=True)\n",
    "        ]\n",
    "        \n",
    "    def _compute_penalty(self):\n",
    "        return torch.norm(I.delta[0], p=2, dim=1)**2\n",
    "    '''\n",
    "        penalty = torch.norm(self.delta[1].view(self.batch_size, -1), p=2, dim=1)**2\n",
    "        #penalty += torch.norm(self.delta[2].view(self.batch_size, -1), p=2, dim=1)**2\n",
    "        #penalty += torch.norm(self.delta[3].view(self.batch_size, -1), p=2, dim=1)**2\n",
    "        #penalty += torch.norm(self.delta[4].view(self.batch_size, -1), p=2, dim=1)**2\n",
    "        #penalty += torch.norm(self.delta[5].view(self.batch_size, -1), p=2, dim=1)**2\n",
    "        #penalty += torch.norm(self.delta[6].view(self.batch_size, -1), p=2, dim=1)**2\n",
    "        return penalty'''\n",
    "\n",
    "    def forward(self, y):\n",
    "        z = self.delta[0]\n",
    "        ys = [y] * len(self.G.blocks)\n",
    "        h = self.G.linear(z) #+ self.delta[1]\n",
    "        h = h.view(h.size(0), -1, self.G.bottom_width, self.G.bottom_width)# + self.delta[2]\n",
    "        \n",
    "        for index, blocklist in enumerate(self.G.blocks):\n",
    "            for block in blocklist:\n",
    "                h = block(h, ys[index])# + self.delta[3+index]\n",
    "        h = torch.tanh(self.G.output_layer(h))# + self.delta[5]\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count for Gs initialized parameters: 4303875\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "trained_path = 'G_cur.pth'\n",
    "G = Generator(n_classes=10, resolution=32, G_shared=False).to(device)\n",
    "G.load_state_dict(torch.load(trained_path, map_location=torch.device(device)), strict=True)\n",
    "for param in G.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = ApproximateEncoder().to(device)\n",
    "E.train(G, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt, label = dataset[7]\n",
    "inpt = inpt.to(device).view(1, 3, 32, 32)\n",
    "y = torch.zeros(1).long().to(device)\n",
    "y[0] = label\n",
    "rn = TruncatedResNet18(device)\n",
    "for param in rn.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_init = E(inpt).detach()\n",
    "I = InvertedGenerator(G, z_init.clone()).to(device)\n",
    "I_optim = optim.Adam(I.delta, lr=0.01)\n",
    "mse_loss = nn.MSELoss()\n",
    "losses = []\n",
    "vis_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20000):\n",
    "    I.zero_grad()\n",
    "    recon = (I(y)+1)/2\n",
    "    Wp = [2, 1, 1]\n",
    "    Gf = [recon, rn(recon, n=0), rn(recon, n=5)]\n",
    "    Re = [inpt, rn(inpt, n=0), rn(inpt, n=5)]\n",
    "    loss = sum([Wp[i]*mse_loss(Gf[i], Re[i]) for i in range(len(Gf))])# + I._compute_penalty()\n",
    "    loss.backward()\n",
    "    I_optim.step()\n",
    "    losses.append(loss.item())\n",
    "    vis_losses.append(mse_loss(recon, inpt).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gcf().set_size_inches(15, 10)\n",
    "\n",
    "plt.subplot(2, 3, (1,3))\n",
    "plt.title('Invertor Losses')\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(vis_losses, label='Reconstruction Loss')\n",
    "plt.plot(np.array(losses)-np.array(vis_losses), label='Residual Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title('Initialization')\n",
    "plt.imshow(img((G(z_init, y)[0]+1)/2))\n",
    "plt.axis('off')\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title('Optimization')\n",
    "plt.imshow(img((recon[0])))\n",
    "plt.axis('off')\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title('Target')\n",
    "plt.imshow(img(inpt[0]))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('reconstruction')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
